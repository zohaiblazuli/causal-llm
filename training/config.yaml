# Training Configuration for Causal LLM Fine-tuning
# Optimized for RTX 4090 (24GB VRAM) - October 2025
# Target: ISEF 2026 (May 2026)
# VRAM Usage Target: 60-65% (~14-16GB / 24GB)

# Model Configuration
model:
  # Base model selection
  name: "meta-llama/Llama-3.2-3B-Instruct"

  # 4-bit quantization (memory efficiency)
  load_in_4bit: true
  bnb_4bit_compute_dtype: "bfloat16"
  bnb_4bit_use_double_quant: true
  bnb_4bit_quant_type: "nf4"

  # Device configuration
  device_map: "auto"
  trust_remote_code: true

  # Max sequence length (INCREASED for RTX 4090)
  max_seq_length: 768  # Was 512 on RTX 3060

# LoRA Configuration (Parameter-Efficient Fine-Tuning)
lora:
  # Core LoRA parameters
  r: 16
  alpha: 32
  dropout: 0.05

  # Target modules for Llama architecture
  target_modules:
    - "q_proj"
    - "v_proj"
    - "k_proj"
    - "o_proj"
    - "gate_proj"
    - "up_proj"
    - "down_proj"

  # LoRA settings
  bias: "none"
  task_type: "CAUSAL_LM"
  modules_to_save: []

# Training Hyperparameters
training:
  # Batch size configuration (OPTIMIZED for RTX 4090 with batched forward pass)
  # NOTE: Batch size 2 × 3 inputs = 6 samples per forward pass!
  per_device_train_batch_size: 2  # Must account for 3x multiplier from batching!
  per_device_eval_batch_size: 2   # Must account for 3x multiplier from batching!
  gradient_accumulation_steps: 8  # Effective batch = 16 (2 * 8)

  # Training duration
  num_epochs: 3
  max_steps: -1

  # Learning rate and scheduling
  learning_rate: 2.0e-4
  lr_scheduler_type: "cosine"
  warmup_ratio: 0.03
  warmup_steps: 0

  # Optimization
  optimizer: "paged_adamw_8bit"
  weight_decay: 0.01
  adam_beta1: 0.9
  adam_beta2: 0.999
  adam_epsilon: 1.0e-8
  max_grad_norm: 1.0

  # Mixed precision training (CRITICAL for memory and speed)
  fp16: false
  bf16: true
  tf32: true

  # Memory optimizations
  gradient_checkpointing: true
  optim: "paged_adamw_8bit"

  # Data loading (OPTIMIZED for RTX 4090)
  dataloader_num_workers: 8  # INCREASED from 4
  dataloader_pin_memory: true
  dataloader_prefetch_factor: 8  # INCREASED from 4

  # Evaluation (CRITICAL: EPOCH-ONLY to prevent validation bug)
  evaluation_strategy: "epoch"  # ← MUST BE "epoch"
  eval_steps: 999999  # ← Safety: Set impossibly high to prevent mid-epoch validation
  save_strategy: "steps"  # Save checkpoints during training for auto-resume
  save_steps: 200  # Save every 200 steps (more frequent for VPS disconnects)
  save_total_limit: 3  # Keep last 3 checkpoints
  load_best_model_at_end: true
  metric_for_best_model: "causal_stability"
  greater_is_better: true

  # Early stopping (DISABLED for unattended training)
  early_stopping_patience: 999
  early_stopping_threshold: 0.001

  # Logging
  logging_steps: 50
  logging_first_step: true
  report_to: []

  # Seed for reproducibility
  seed: 42
  data_seed: 42

  # Misc
  remove_unused_columns: false
  ddp_find_unused_parameters: false

# Loss Configuration (FULL CAUSAL LOSS)
loss:
  type: "causal_contrastive"

  # Loss weights
  lambda_task: 1.0
  lambda_causal: 0.5
  lambda_spurious: 0.5

  # Contrastive learning
  temperature: 0.07
  similarity_metric: "cosine"

  # Task loss
  label_smoothing: 0.0

# Data Configuration
data:
  # Data paths
  train_path: "data/processed/train_split.jsonl"
  val_path: "data/processed/val_split.jsonl"
  test_path: "data/processed/test_split.jsonl"

  # Data processing (INCREASED for RTX 4090)
  max_length: 768  # Was 512 on RTX 3060
  padding: "max_length"
  truncation: true

  # Cache
  use_cache: true
  cache_dir: "data/cache"

  # Data validation
  min_length: 10
  max_samples: -1

# Checkpointing
checkpointing:
  output_dir: "checkpoints"
  resume_from_checkpoint: null
  save_safetensors: true

# Weights & Biases Configuration
wandb:
  enabled: false
  project: "isef-causal-llm"
  entity: null
  name: null
  tags: ["llama", "lora", "causal-contrastive", "rtx4090", "isef2026"]
  notes: "ISEF 2026: RTX 4090 optimized training with auto-resume"
  group: null

# Validation and Testing
validation:
  # Compute causal metrics during validation
  compute_causal_metrics: true
  compute_attack_success_rate: true

  # Sample generation during validation
  generate_samples: true
  num_samples_to_generate: 5
  generation_max_length: 256

# Debug and Development
debug:
  # Debug mode (smaller dataset, faster iterations)
  enabled: false
  max_train_samples: 100
  max_eval_samples: 20

  # Memory profiling
  profile_memory: false
  memory_snapshot_path: "memory_snapshots"

  # Detect anomalies
  detect_anomaly: false

# Hardware-Specific Settings for RTX 4090
hardware:
  gpu_name: "RTX 4090"
  vram_gb: 24
  cuda_device: 0

  # Memory management (OPTIMIZED for RTX 4090)
  empty_cache_steps: 50  # DECREASED from 100 (4090 is faster)
  max_memory_allocated_gb: 16.0  # Target ~67% utilization, not 100%

  # Performance tuning
  use_flash_attention: false
  use_triton: false
  compile_model: false

# Advanced Settings
advanced:
  # Gradient accumulation strategy
  gradient_accumulation_strategy: "steps"

  # DDP (Distributed Data Parallel) settings
  ddp_backend: "nccl"
  ddp_timeout: 1800

  # Sharding (for multi-GPU, future-proofing)
  fsdp: false
  fsdp_config: null

  # DeepSpeed (alternative to FSDP)
  deepspeed: null

  # Torch settings (OPTIMIZED for RTX 4090)
  torch_dtype: "auto"
  attn_implementation: "sdpa"
