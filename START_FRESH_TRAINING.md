# Start Fresh Training From Beginning

**Date**: October 17, 2025
**Status**: âœ… Ready to train from scratch
**All bugs fixed**: Yes (committed in fd1c5cc)

---

## Quick Start on VPS

```bash
cd causal-llm

# Pull latest fixes (all training loop bugs fixed)
git pull origin main

# Start fresh training (NO --resume flag)
python training/train.py --config training/config.yaml
```

**That's it!** Training will start from epoch 1, step 0.

---

## What Changed

**All training loop bugs have been FIXED**:
1. âœ… Mid-epoch validation now respects `evaluation_strategy: "epoch"`
2. âœ… Progress bar shows correct percentage
3. âœ… Callbacks only trigger on processed batches
4. âœ… Epoch metrics calculated correctly
5. âœ… No more continuous validation after batch 3999

**Config is already set to NOT resume**:
- `resume_from_checkpoint: null` (line 156)
- Simply DON'T use `--resume` flag when running train.py

---

## Expected Training Behavior

### Fresh Training (3 epochs):
```
Epoch 1/3: Train 7151 batches â†’ Validate at end
Epoch 2/3: Train 7151 batches â†’ Validate at end
Epoch 3/3: Train 7151 batches â†’ Validate at end
Total validations: 3 (one per epoch)
```

### On A100 (40GB):
- **Duration per epoch**: ~8-10 minutes
- **Total training time**: ~25-30 minutes
- **Validation time**: ~4 minutes per epoch
- **Total**: ~37-42 minutes

### On V100 (32GB):
- **Duration per epoch**: ~10-12 minutes
- **Total training time**: ~30-36 minutes
- **Total**: ~42-48 minutes

---

## What You'll See

```
================================================================================
CAUSAL LLM TRAINING - RTX 4050 Optimized
================================================================================

Configuration loaded from training/config.yaml
Random seed set to 42

--------------------------------------------------------------------------------
MEMORY ESTIMATION
--------------------------------------------------------------------------------
Estimated memory usage:
  model: 2.34 GB
  optimizer: 0.45 GB
  gradients: 0.23 GB
  activations: 0.89 GB
  total: 3.91 GB

Memory estimate looks good! (3.91 GB / 40 GB)

--------------------------------------------------------------------------------
MODEL SETUP
--------------------------------------------------------------------------------
Loading model: meta-llama/Llama-3.2-3B-Instruct
Base model loaded
After loading base model: Memory: 2.45 GB allocated, 5.23 GB reserved
LoRA applied with rank 16
Causal projection moved to device: cuda:0
Trainable parameters: 12,845,056 / 3,213,824,000 (0.40%)
After applying LoRA: Memory: 2.56 GB allocated, 5.34 GB reserved
Gradient checkpointing enabled

--------------------------------------------------------------------------------
DATA LOADING
--------------------------------------------------------------------------------
Loading datasets...
Train dataset: 7151 samples
Val dataset: 893 samples

--------------------------------------------------------------------------------
TRAINING SETUP
--------------------------------------------------------------------------------
Optimizer: PagedAdamW8bit
Loss function: CausalContrastiveLoss

Callbacks: ['ProgressLogger', 'LearningRateMonitor', 'MemoryMonitor',
            'CausalMetricsLogger', 'ModelCheckpoint']

================================================================================
STARTING TRAINING
================================================================================
Before training: Memory: 2.67 GB allocated, 6.12 GB reserved

Starting training for 3 epochs
Total training steps: 2682

Epoch 1/3:   0%|                    | 0/893 [00:00<?, ?it/s]
Epoch 1/3:   1%|â–                   | 8/893 [00:05<09:30, 1.55it/s, loss=14.23, causal=0.456, spurious=0.321]
...
Epoch 1/3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 893/893 [08:45<00:00, 1.70it/s]

Running validation...
Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 893/893 [03:45<00:00, 3.96it/s]

Validation Results:
  Loss: -14.2851
  Causal Stability: 1.0000
  Spurious Separation: 2.0000
  Causal Discrimination: 2.0000

Validation causal metrics:
  causal_stability: 1.0000
  spurious_separation: 2.0000
  causal_discrimination: 2.0000

Causal projection saved to checkpoints/best_model/causal_projection.pt
Checkpoint saved to checkpoints/best_model
Validation causal stability improved to 1.0000

Epoch 2/3:   0%|                    | 0/893 [00:00<?, ?it/s]
...
```

---

## After Training Completes

You'll have:
```
checkpoints/
â”œâ”€â”€ best_model/                    # Best checkpoint (highest causal stability)
â”‚   â”œâ”€â”€ adapter_model.safetensors  (~160MB)
â”‚   â”œâ”€â”€ adapter_config.json
â”‚   â”œâ”€â”€ causal_projection.pt       (~75MB)
â”‚   â”œâ”€â”€ trainer_state.pt           (~85MB)
â”‚   â””â”€â”€ tokenizer files
â”œâ”€â”€ checkpoint-epoch-1/            # Epoch 1 checkpoint
â”œâ”€â”€ checkpoint-epoch-2/            # Epoch 2 checkpoint
â””â”€â”€ checkpoint-epoch-3/            # Epoch 3 checkpoint (final)
```

---

## Next Steps After Training

1. **Download the trained model** (if on VPS):
   ```bash
   # On VPS, compress the checkpoint
   tar -czf best_model.tar.gz checkpoints/best_model/

   # On local machine, download
   scp user@vps:causal-llm/best_model.tar.gz ./
   tar -xzf best_model.tar.gz
   ```

2. **Run evaluation** (Phase 2):
   ```bash
   python evaluation/benchmark.py --model checkpoints/best_model
   ```

3. **Proceed to Phase 3**: Formal verification (PC/GES, HSIC tests, PAC bounds)

---

## Troubleshooting

**If you see mid-epoch validation:**
- Check: `evaluation_strategy: "epoch"` in config (line 91)
- Verify: Using latest code from GitHub (commit fd1c5cc or later)

**If training seems slow:**
- Expected: ~1.5-1.7 it/s on A100
- Check: `nvidia-smi` shows GPU at 95-100% utilization
- Memory: Should use ~4-6GB out of 40GB on A100

**If you get OOM errors:**
- Reduce: `gradient_accumulation_steps: 4` (from 8)
- Reduce: `max_length: 384` (from 512)

---

**Status**: All systems ready for fresh training! ðŸš€
